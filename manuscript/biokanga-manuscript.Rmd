---
title: 'A fully reproducible and extensible evaluation framework for RNA- and DNA-Seq aligners and assessment of biokanga - a general purpose bioinformatics toolkit'
author:
- Author First (University of Foo)
- Another Person (University of Bar)
output:
  bookdown::pdf_document2:
    number_sections: yes
    toc: false
    # keep_tex: yes #if needed for final submission
abstract:
  This is the abstract.
bibliography: references.bib
csl: elsevier-harvard.csl
link-citations: true
urlcolor: blue
linkcolor: red
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.path = "figures/"
)
```


# ideas/stubs
- good enough practices paper
- Sandve GK, Nekrutenko A, Taylor J, Hovig E. Ten Simple Rules for Reproducible Computational Research. PLoS Comput Biol. 2013;9(10). pmid:24204232
- Brett K Beaulieu-Jones & Casey S Greene Reproducibility of computational workflows is automated using continuous analysis
- [Developing a modern data workflow for regularly updated data](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000125)
- [Practical Computational Reproducibility in the Life Sciences](https://doi.org/10.1016/j.cels.2018.03.014)
- [Methods for enhancing the reproducibility of biomedical research findings using electronic health records](https://doi.org/10.1186/s13040-017-0151-7)
- [Systematic benchmarking of omics computational tools](https://www.nature.com/articles/s41467-019-09406-4) - a Review
- [Critical Assessment of Metagenome Interpretation - a benchmark of metagenomics software](https://www.nature.com/articles/nmeth.4458) - with containers and Zenodo
- [Terminologies for Reproducible Research](https://arxiv.org/pdf/1802.03311.pdf)


# Introduction

- Why does computational reproducibility matter? Point to refs, needs to very brief
- Value in reproducibility for analysis generally, and benchmarking
- Challenges around reproducibility/recomputability in computational biology. Usually more than one tool make things hard. Greater complexity and computation intensiveness increases these challenges.
- Beyond these challenges, how can we make work extensible and reusable?

### key concepts

- Using thinking around devops (CA paper) can leverage improvements from software engineering for research/analytical code to make it deployable and reusable
- Key concepts:
  + code under git version control system with well-defined use of git and GitHub functions and features to address some of the challenges listed above
  + environment/dependencies
  + building/exposing containers
  + tracking releases and linking to artefacts
  + minting of DOIs for code, output and execution metadata
  + workflow management
    + description of workflow in a standard way
    + sharing of workflow - can be re-run by others with minimal effort
    + separation of pipeline logic from execution environment - (potential for) portability across execution environments
    + scalability "for free" through implicit parallelisation - independent tasks will be executed separately if resources available

### This work
- In an effort to demonstrate a framework that implements these principles of reprocomputability, extensibility and transparency, we chose to implement an entirely existing toolset in the context of previously published work, in an effort to highlight the fact that the technology and support already exists to make these practices the norm - the challenge is orchestrating the range of tools to enable a complete solution. We have chosen the space of DNA/RNA aligner benchmarking so that we can illustrate these approaches in the description of the BioKanga suite of tools, specifically its aligner.

### Previous work
Previous efforts related to aligner benchmarking published a code repository making it technically possible to reproduce the analysis, however several factors meant that it was challenging to extend and reuse.
Even though versions of the required software were specifically stated, re-running of the analysis would require downloading and installing all the required software tools (assuming they are availiable).
Furthermore, the original analyses were carried out in specific directory structure which could not be reproduced on another system by an unprivileged user.
The alternative would be to laboriously edit the multitude of hard-coded paths while not being able to test the code until sufficient changes have been made to make it functional again.

### biokanga align
- Introduce biokanga

# Methods

Our approach relies on several key systems used in concert.
Use of GitHub offers much more than version control as it offers programmatically accessible spaces
for other systems to/from which deposit/retrieve information.

## Scalability

* NF implicit parallelization
* Flexible - will make good use of resources whether on a single multi-core machine or a cluster of thin nodes

## Extensibility

Additional aligners can easily be included in and evaluated by the workflow.
This is facilitated by the pipeline's modular design, the main script which describes
the pipeline logic remains unchanged and addition of an aligner is done through
inclusion of a simple genome indexing template and one or two alignment templates
depending on whether the tool can be configured to align both DNA- and RNA-Seq
reads.
The final addition is that of a dedicated container image
as we outline in the [next section](#Building-and-exposing-containers).





## reproducibility practices


### Building and exposing containers

A single container image capturing the software environment for a workflow should suffice in most cases.
For some workflows however, it may be preferable to have a separate container image for each tool.
In the case of our workflow, this enables easy addition and benchmarking of a new tool
while the rest of the workflow remains unaffected, with the exception of some downstream processes
which aggregate data produced by all of the evaluated tools.
Such processes need to be re run regardless.
Having a separate container per tool or perhaps per branch of the workflow may also be advantageous
for computationally intensive or long-running workflows as this approach limits the need to
re-run the  entire pipeline or its substantial parts due to global software environment being modified.

Meaningful benchmarking requires precise traceability of each evaluated software tool.
The minimum is the stating of the version of the tool being used and perhaps providing
the appropriate binary executable.
More broadly, reproducibility of computational analyses demands that we not only capture
the version of a given tool but the entire software environment.
To address this and the additional goals of reusability and extensibility,
we propose procedures for building, distributing and using the containers.

For a handful of benchmarked tools we rely on containers from BioContainers collection.
For most of the remaining tools however, we provide a dedicated Docker Hub repositories.
Each repository is configured to automatically build a version of the container
for a tool, given an appropriate trigger in our workflow's GitHub repository.

Additional docker container images are provided with auxiliary tools required for benchmarking,
including, but not limited to the RNA-seq benchmarking toolkit [@Baruzzo2016] (modified in https://github.com/rsuchecki/biokanga_benchmark)
and the DNA-seq benchmarking toolkit [@Brinda2015] (builds automated as for benchmarked tools).

The procedure for adding or updating individual container images
involves creating a dedicated `tool/version` branch in the main repository, for example `samtools/1.9`.
Changes committed to that branch are captured in a container image in the associated Docker Hub repository.
This is done through an automated build triggered by code being push to that branch on GitHub.
The generated container image is tagged with the version specified in the git branch name.
Since a container image captures more than just the primary tool of interest, the tool version tag
is not sufficient from reproducibility perspective.
A container image may have to be re-created for a number of reasons other than version change of the main tool.
An underlying base image may need to be patched or an auxiliary tool may have to be added to the container
to facilitate its use within a workflow e.g. to aid collection of execution statistics.
In such case the version tag remains unchanged even though the underlying container image has been replaced.
We provide automation code (a "`post_push` hook") which ensures that in addition to the container image version tag, which may potentially be overwritten,
additional tags based on git commit SHA hash are generated.
These effectively permanent tags are then used when referencing a container in workflow configuration files.
For example, rather than pointing to `1.9` tag in our `samtools` Docker Hub repository we point to `1.9_ea5d3c82fb85c174dce08a4c736ed44c1e6bb7eb`.
The permanence of such created container images relies on the procedure being followed.

Docker v2 specification provides another (partial?) solution of addressing the problem
of container image permanence (?) via [content digests](https://docs.docker.com/registry/spec/api/#content-digests).
A digest contains a SHA-256 collision-resistant hash of the content of an image.
Implemented by Docker registries such as Docker Hub or Quay,
a digest is returned when an image is pushed to a repository.
The digest can later be used to pull this exact version of the container image.
We use this functionality to reference the BioContainer images.
For example
`quay.io/biocontainers/subread@sha256:a27dcc4f335d8f98346a9b9d886230de6bba2bf7518372c2022c22ab225c09ff`.
Our container images hosted on DockerHub could be accessed in the same fashion.
However, specification of a container image is clearer if a tag containing the version number is used,
so the `repository:tag@digest` is optimal as in addition to having an explicit version tag,
the image is only pulled if the specified digest matches the one in the registry.
Our workflow is compatible with this syntax, but Singularity currently (as of version 3.1.1) does not support it.
Since we already have a mechanism for ensuring permanence of container images we opt for
not applying this approach to maintain the portability of the workflow - unlike Docker, Singularity is commonly available on HPC clusters.

We maintain support of Singularity for portability of our workflow
but currently not for reproducibility since we only specify Docker containers
and each needs to be converted to a Singularity container.
This conversion is supposedly not fully consistent,
although this is apparently due to potential change to the layers in the input Docker container image.
Since we ensure the stability of those container images, this should not be an issue.
If we were to use Singularity directly for reproducibility of our results
we would opt for providing and maintaining Singularity container images
on Singularity Hub the means of insuring the stability of the containers used.
We provide an example of how this can be achieved
in [https://github.com/rsuchecki/miniconda3](https://github.com/rsuchecki/miniconda3).
Briefly, for a given software tool a repository needs to be created on Singularity Hub
and linked to a GitHub repository containing the recipe for creating the container image.
With the already available Docker container image, it suffices
for the corresponding Singularity recipe to simply point
to the Docker container image as its source.
Building of the container image is triggered by changes made to the GitHub repository
and a fixed version of the container image remains available from Singularity Hub.
Functionality and flexibility of Singularity Hub is currently somewhat limited
as compared to Docker registries which makes it less suited for the type of work
we present here. For a project relying on fewer containers it could already be
an equally good and potentially a superior solution, given for example,
the ability to easily freeze a container image build/tag, to ensure it will not overwritten.





### Capturing code and associated results under DOIs

A particular revision of code in a  git repository
can be tagged to clearly identify a version which may be deemed important.
It is standard practice in software development to tag versions
using semantic versioning e.g. `v1.0.5` the same style may be, and often is (REF) used for scientific workflows.
The metadata describing the significance of a tag may be of more importance,
this could include a high-level description of the changes introduced to code
but also scientific context relevant to these changes.

The concept of GitHub releases extends that of git tags such that extra information and data can be included.
In addition to capturing a snapshot of code used to generate some results we are able to capture
workflow run metadata and the generated results.
GitHub does not limit the total size of binary release files,
but each individual file must be under 2 GB in size.
Larger result files can be split or GitHub Large File Storage can be used.

Using integration between Zenodo and GitHub where our git repository is hosted,
DOIs are generated for each GitHub release.
The end product is a DOI for the project, but also a DOI for each release.
Given an (optional) authentication token, our workflow, on
completion, using API calls, can create a GitHub release associated with that run,
upload workflow metadata and results and finally trigger DOI minting on Zenodo.

Others can use this functionality by simply forking our repository,
generating an access token in their GitHub account and linking their Zenodo account
with the forked repository.

### Git version control and workflow sharing

Keeping code under version control and tagging notable versions as described above
paves the way to workflow sharing. We already have the code for specific revision
available with execution metadata and the results generated by this run/revision.
The particular revision can e checked-out from the git repository, ready for execution.

Nextflow provides a convenient shorthand for that, where a revision can be
(re-) executed e.g. by specifying, at runtime, a version tag (e.g. `v1.4`).
Nextflow will download the specific revision before executing the workflow.

To allow others to re-run the workflow with minimal effort,
we rely on Nextflow facilitating separation
of workflow logic from execution and software environment.
We  distribute our workflow with several execution profile
configurations which allow for the pipeline to be run on a dedicated server,
a high performance computing (HPC) cluster or in the cloud.
Crucially, the workflow logic remains unchanged while the software environment
is provided either via docker or singularity containers depending on execution
profile used.
Additional execution profiles can be added to accommodate other compute environments.

By far the most portable execution profiles are those relying on public cloud resources.
This facilitates practical reproducibility, allowing anyone with sufficient means
to rent such resources from a cloud provider,
to re-run, confirm or challenge postulated results with negligible effort.

Our `awsbatch` profile requires the user to have and AWS account and
AWS Batch compute environment configured - this is not specific to our workflow
but we document the configuration steps required at
[https://github.com/csiro-crop-informatics/biokanga-manuscript](https://github.com/csiro-crop-informatics/biokanga-manuscript).
The advantage of using this profile is that it allows for workflow execution configuration
to be defined consistently for users who otherwise would only have access to distinct compute environments.
Specific compute instance types (hardware) can be set to facilitate reproducibility although in practice
it might be preferred to allow for heterogeneous composition of compute resources
such that resources use is better optimized and the compute costs are suppressed.

All jobs run on AWS batch must be run in containers, so unlike
in HPC environment where some processes may rely on commonly available tools
pre-installed on the system, we are forced to explicitly configure the software environment.
In conjunction with other practices proposed in this work, this ensures that
a complete record of software used is preserved and readily available for re-use.

# Reproducibility overhead

Building workflows for reproducibility creates a number of challenges for the developer,
depending on developer's past experience and expertise these may include the need for mastering
new concepts, new tools and potentially also a new programming language at least up to a point
required to glue scripts and programs into a coherent whole.

Containerisation of software environment means that some of system administration responsibilities
are effectively transferred to the developer who, for example, needs to ensure that the entire tool-set
required for a given task is included in the container.
This may include software tools which we take for granted in a HPC environment but may not be available
in source images commonly used for building custom containers.

Portability of workflows across execution environment can be difficult to achieve.
Main risks lie in the ease of a quick and often compute-specific solutions to problems arising in the development.
Nextflow enables the separation of workflow logic and configuration but it is not enforced as this would
be detrimental to flexibility and usability of the system.
Given execution environments may pose some constraints which have to be addressed and as a result,
additional code is required to ensure that workflow behaviour conforms to those constraints.
For example, when running the workflow in the cloud (AWS Batch), input/output locations for data and workflow metadata
have to be handled differently the in the case of a HPC cluster.
This is done seamlessly by our workflow, but required additional time invested upfront to develop and test
an appropriate configuration.

Although substantial, all these overheads can be seen as investments.
Carefully designed portable workflow configuration can be easily re-used
such that subsequent workflows can be developed much quicker.
The container images tagged with respective tool versions and commit SHA hashes
are freely available, so anyone who needs to use the specific version of a tool,
can simply point to an image in their workflow definition or pull, build and use the  container directly.


* consider also modularity
  * NF under dev
  * Snakemake wrappers
















## describe workflow

### Use of containers

The workflow relies on multiple containers executed either through Docker or Singularity.
Core to the workflow design is having a separate container for each aligner.
Additional containers provide the tool-sets required to pre- post-process data
and evaluate results. For example the RNFtools container is used to simulate
the DNA-Seq reads and later to evaluate the alignment results.



## describe biokanga align

# Results

- show benchmark outputs
- nextflow graph?

# Discussion

- Use of a framework such as this allows for straightforward and consistent deployment of analytical code and tools
- Clarity around which version and environment was used to produce a particular artefact/output
- extensible (guide for adding an aligner is clear)
- biokanga is a reasonable option as an aligner

# Conclusion

- A comprehensive approach such as the one presented here requires a non-trivial amount of overhead
- The advantages are significant
- The framework can be reused very simply (the overhead doesn't need to be spent by someone else)
- Methodology developed e.g. for container version tracking and deployment can easily be applied elsewhere.

[//]: # (Cite papers like this: `[@Kim2015]` [@Kim2015; @Baruzzo2016] matching entries in `references.bib`.)

BioKanga is a general purpose bioinformatics toolkit targeting many utilised within bioinformatics workflows.
We present BioKanga toolkit in the context of a turn-key reproducible and easily extendible evaluation frameworks for RNA-Seq and DNA-Seq.
We demonstrate how BioKanga `align`, a general purpose short read aligner performs on par with specialist RNA-Seq aligners
and that by some measures general purpose aligners may outperform the specialised ones.
We also show how design decisions behind BioKanga align lead to more robust results then those produced by popular alignment tools such as bwa or bowtie2 when aligning DNA-Seq reads to large, complex and repetitive polyploid genomes.

## BioKanga toolkit

Individual BioKanga tools are integrated within a single executable process.
The tools are presented as user selected submodules, with each submodule exposing a set of parameterisation defaults which can be overridden as appropriate given a priori knowledge.
Although a general purpose toolkit, the majority of individual BioKanga submodules have proven to be very competitive with other specialised  programs individually targeting individual bioinformatics tasks. BioKanga is developed in C and is available at [https://github.com/csiro-crop-informatics/biokanga](https://github.com/csiro-crop-informatics/biokanga).
This paper gives an overview of the `align` submodule and evidences the competiveness of the BioKanga align submodule relative to one of the specialised RNA-seq aligners, HISAT2 [@Kim2015], as recently benchmarked by Giacomo Baruzzo et al. [@Baruzzo2016].

## BioKanga align within RNA-Seq benchmarking framework

A major task in most NGS dataset dependent bioinformatics workflows is the alignments of the reads against targeted sequences.
Such targets may be in the form of a reference fully assembled genome comprising multiple chromosomal sized sequences, partially assembled genomes comprising scaffolded contigs, transcribed gene sequences, or in fact any arbitrary set of sequences having some experimental relevance.
The NGS datasets may themselves have been generated using a variety of technological resources with different library preparation protocols for which there will be different optimal best alignment strategies required.
The biokanga alignment submodule, `biokanga align`,exposes a large number of internal thresholds and alignment strategies allowing the experimenter to select the most optimal set of parameters which maximise their chances of meeting or exceeding experimental objectives.
It must be noted that there are defaults for most of the parameterisation thresholds which have been chosen empirically and in most cases can deliver alignments which optimally target experimental objectives.
In the benchmarking experiment published by Giacomo Baruzzo et al. [@Baruzzo2016], two sets (human and malaria) of 100bp paired end BEERS [@Grant2011] simulated reads containing 3 different complexities of error rates were generated with these sets consisting of simulated reads drawn from merged combinations of selected known transcriptomes wherein exons of individual transcripts have been mapped on to the genomic DNA assemblies.
In addition to the simulated reads the BEERS [@Grant2011] reads simulator also outputs the genomic DNA loci of where individual reads were sourced from including splice junctions spanned by these reads with this information defined as comprising the ‘ground truth’.
Alignments were generated by each of the benchmarked aligners using the total genomic DNA assemblies as targets with the loci of resultant alignments then compared against the BEERS reported ground truth loci.
From this comparison, various statistics by the Giacomo Baruzzo et al [@Baruzzo2016] benchmarking framework are then reported including proportions of incorrectly and correctly aligned reads and bases.
Using the same benchmarking framework [@Baruzzo2016] and BEERS simulated reads, BioKanga align was used to compare the qualitative performance of using the general purpose BioKanga toolkit aligner, using RNA-seq parameterisation, against the specialised RNA-seq aligners, many of which were used in the original benchmarking [@Baruzzo2016]. We have also included additional aligners, both specialised and genral purpose.

The BEERS simulated reads have a number of deficiencies in that these generated reads do not realistically approximate reads with the error profiles as normally observed in Illumina PE RNA-seq sequenced datasets but decided to use the BEERS simulated reads so that the unchanged benchmarking framework result sets as published by Baruzzo et al. can be used for comparison with Biokanga alignments.
One change which was made was to the supplementary simulated read sets incorporating partially retained adapters.
These retained adapters were only incorporated at the 3' end of reads (retention is normally at the 5'), were of fixed length (multiples of 10), only the 1st 1 million (10%) read pairs had retained adapters and both ends of these read pairs had exactly the same length retained adapter.
The retained adapter reads were generated by the `add_adapter2fasta_V2.pl` Perl script in the benchmarking framework.
A new script was written with a more realistic distribution of retained adaptor lengths with adaptors, where retained, only at the 5' end of reads.
All aligners were then benchmarked against these improved retained adapter datasets.

# Retained Adaptors and Artefact Sequences Confound Aligners

It is very common for NGS reads to contain artefact sub-sequences at the 5' and/or 3' read ends which have originated as a by-product of library preparation prior to actual base calling or inadequate adaptor trimming post sequencing by the sequencing dataset provider.
A combination of methods are generally employed for reduction of these artefact sub-sequences prior to alignment of reads against the targeted genome assembly. With RNA-seq reads the target may be either genomic or transcriptomic.
The generally employed methods use a set of known artefact sequences, e.g. adaptor sequences, which when detected at the 5' or 3' of a read will result in that read being hard trimmed until the known artefact sequence is no longer detectable.
Some methods simply hard trim a fixed number of nucleotides off. The question naturally arises as to what K-mer length of a putative artefact sequence must be present at the 5' or 3' end of a read before there can be confidence that the k-mer sequence of nucleotides is part of an artefact sequence rather than a naturally occurring K-mer sequence within the targeted assembly or transcriptome.
Additionally, sequenced artefacts will have the same background base call error rate profile as the sequenced targeted fragments.
Thus, it is common to require a relatively large K-mer of known artefact sequence to exactly match at the 5' or 3' end of a read before trimming that read to decrease the false positive rate.
Post trimming, during alignments to the targeted genome or transcriptome, a high substitution rate is required to gain alignments with those reads still containing end artefacts less than the k-mer detection threshold and/or which contained base calling substitution errors.
A high substitution rate may also result in reads which did not contain end artefact sequences being misaligned unless care is taken to only select alignments with minimal required substitutions.
A further confounding factor is that the artefact nucleotide sequences themselves may not be known as would be the case with cross over or chimeric joining during library PCR.

# BioKanga Utilises Adaptive Chimeric Trimming
BioKanga provides the implementation of a method whereby alignments are made without the need to firstly detect and trim end artefact sequences before actual alignment processing.
The degree of end trimming is adaptive so as to maximise the read alignment length whilst minimising the number of required substitutions along the alignment length.
This combination of maximising alignment length together with minimising required substitutions maximises confidence in the resultant accepted alignments.
here is no requirement to have prior knowledge of where in the sequencing phases the artefacts were introduced or the actual nucleotide sequences of these artefacts.
 The only constraint is that the artefacts must be at the 5' and/or the 3' ends of reads.
BioKanga will explore all putative alignments for a given read and select as aligned that alignment, from all putative, having maximal alignment length with minimal substitutions over the alignment length.
Maximal alignment length is prioritised over minimal substitutions.
With paired end read datasets the reads individually can be adaptively chimeric trimmed before normal paired end processing commences.

The relative success of this approach is exemplified by the performance of BioKanga within a benchmark framework originally targeting RNA-seq aligners whereby BioKanga accurately aligns more reads than HiSat2 which was one of the top performing aligners in the original study.
Adaptive end trimming enables BioKanga to trim RNA-seq reads back over splice junctions into exons when aligning RNA-seq against genomic targets containing introns.

# Adaptive Chimeric Trimming Method

BioKanga will first attempt to align reads without any trimming using the maximum number of allowed substitutions as specified by the user.
If there are multiple full length putative alignments for a given read then BioKanga will accept only that alignment requiring the least number of substitutions.

Reads remaining unaligned are then processed as follows:

a) $K = L/(S-1)$, where L is read length and $S$ is the number of allowed substitutions proportional to the read length, specifies the length of core sub-sequences to be segmented from each read. A given read will provide $N = (L+K-1)/K$ K-mer cores, with the Nth 3' right hand core partially overlapping with the previous core when $(L+K-1)/K$ is not integral.
b) Cores are iterated from 1 (5' left most) up to N (3' right most) with each K-mer core exact matched, no substitutions allowed, against the targeted sequences. A given read may have zero or more core K-mers with exact matches, and each matching core K-mers may be aligning to multiple target loci.
c) Each exactly matching K-mer core is then extended left and right within the boundaries of the originating read length with extension stopped after accumulating mismatches up to the maximum number of allowed substitutions. All alignment loci, sense, alignment length, and number of substitutions required are noted for each K-mer core.
d) The original unaligned read is then reverse complemented to enable antisense alignments to targeted sequences and steps b) through c) inclusive are repeated to derive both sense and antisense core extensions.
e) The best alignment loci chosen for a given read will be that extended K-mer core in each read which resulted in an alignment with maximal extended length and having the least required substitutions proportional to the alignment extended length.

The net result of the forgoing processing is that reads with end artefact sub-sequences will be adaptively trimmed without requiring prior knowledge of these artefacts or their nucleotide sequences whilst minimising the number of nucleotides required to be trimmed from each read.

# Benchmark Methods

To make our results readily reproducible, we have developed a Nextflow [@DiTommaso2017] pipeline which seamlessly handles tasks ranging from data download through execution of the aligners to summarising the results.


## Simulated RNA-Seq data

Benchmarking scripts from Baruzzo et al. have been encapsulated in a Docker container together with some key software dependencies and are executed by the Nextflow pipeline using either Docker or Singularity.
The Nextflow pipeline automatically downloads test datasets (complexity levels T1, T2, T3, human only) generated with the BEERS simulator and made available by Giacomo Baruzzo et al. at http://bioinf.itmat.upenn.edu/BEERS/bp1/datasets.php.
Complexity level T1 is documented as having a substitution rate of 0.001, indel rate of 0.0005 and error rate 0.005.
Complexity level T2 is documented as having increased substitution rates of 0.005, indel rates of 0.002 and error rates 0.01.
Complexity level T3 further increased substitution rates to 0.03, indel rates to 0.005 and error rates to 0.02.
T3 also has an error rate of 0.5 in the last (3') 0 bases.
It is also stated that approximately 40% of reads involve introns with PE insert size distributions having minimum length 100bp, mean 200bp and maximum of 500 bp.
The benchmarking framework was downloaded from [http://bioinf.itmat.upenn.edu/BEERS/bp1/scripts/Alignment_scripts.zip](http://bioinf.itmat.upenn.edu/BEERS/bp1/scripts/Alignment_scripts.zip) and [http://bioinf.itmat.upenn.edu/BEERS/bp1/scripts/aligner_benchmark_khayer.zip](http://bioinf.itmat.upenn.edu/BEERS/bp1/scripts/aligner_benchmark_khayer.zip) in accordance with the download instructions presented by the benchmarking authors  at [http://bioinf.itmat.upenn.edu/BEERS/bp1/software.php](http://bioinf.itmat.upenn.edu/BEERS/bp1/software.php)

A new script `add_adapter2fasta_V3.pl` was developed to fix the issues with adaptor retentions experienced when using datasets generated by the pre-existing `add_adapter2fasta_V2.pl`.

## Real RNA-Seq data

In addition to the simulation-based evaluation, our pipeline includes an execution path where real RNA-Seq reads are aligned using exactly the same tools, indices and parametrisation as in the case of the simulation experiments.
In this case we are unable to evaluate the correctness in terms of read alignment locations but these results provide more realistic run-time comparison independent of potential artefacts introduced by read simulation.
In addition, considering reported alignments of real data alongside a given tool's accuracy measures provides some perspective for interpreting

* alignment rates reported by a given aligner
* consistency of some of simulation-based results with those obtained using real reads


# Acknowledgements {-}

[https://github.com/Robinlovelace/rmarkdown-paper-repo](https://github.com/Robinlovelace/rmarkdown-paper-repo)

# References

