manifest {
  name = 'Biokanga Eval'
  version = 'not set'
  author = 'Rad Suchecki et al.'
  homePage = 'https://github.com/csiro-crop-informatics/biokanga-manuscript'
  description = 'Reproducible aligner evaluation workflow'
  defaultBranch = 'master'
  nextflowVersion = '!>=19.02.0-edge' //due to https://github.com/nextflow-io/nextflow/issues/1015 fix which prevents failure when (internal) list of either RNA or DNA aligners empty
}

// // Global default params, used in configs
params {
  release = false //if true and GH_TOKEN set in environment and executed from repo then (1) create release, (2) upload results and meta artefacts (3) finalise release to trigger zenodo build
  outdir = "./results"
  infodir = "./flowinfo"
  publishmode = "copy"
  debug = false  //set true for quicker runs on subset data
  mode = 'realRNA|simulatedDNA|realDNA|simulatedDNA'
  // email = '' //multiple comma spearated allowed - just use -with-notification
  //RNA alignment
  alignersRNA = 'all' //use --alignersRNA 'bwa|biokanga|dart' for specific subset, or e.g. --alignersRNA '^((?!bbmap).)*$' to exclude
  //DNA alignment
  alignersDNA = 'all' //use --alignersDNA 'bwa|biokanga|dart' for specific subset, or e.g. --alignersDNA '^((?!bbmap).)*$' to exclude
}
//Default inputs for Simulated DNA
includeConfig 'conf/simulatedDNA.config'

//Default inputs for Simulated RNA
includeConfig 'conf/simulatedRNA.config'

//Output publishing conf
includeConfig 'conf/publish.config'

//Compute requirements, CPUs, RAM, time
includeConfig 'conf/requirements.config'

//Process specific container conf
includeConfig 'conf/containers.config'

process {
  cache = 'lenient'
  errorStrategy = params.debug ? 'finish' : 'retry'
  maxRetries = 4
}

profiles {
  docker {
      docker {
        enabled = true
        fixOwnership = true
    }
  }
  awsbatch {
    aws.region = 'ap-southeast-2'
    process {
      executor = 'awsbatch'
      queue = 'flowq'
      // container = 'rsuchecki/tools:0.2' //only need standardi-ish linux toool set: wget, gawk... //replaced by individual containers where applicable
    }
    executor {
      awscli = '/home/ec2-user/miniconda/bin/aws'
    }
  }
  slurm {
    process {
      executor = 'slurm'
      scratch = false
      withLabel: 'rscript|rrender' {
        executor =  'local'
      }
    }
  }
  singularity {
    singularity {
      enabled = true
      autoMounts = true
      cacheDir = "${HOME}/singularity-images"  //when distibuting the pipeline probably should point under $workDir
    }
  }
  singularitymodule {
    process.module = 'singularity/3.1.1' //Specific to our cluster - update as required
  }

}


// //used only to spin out a "head node" on EC2 to then run the pipeline from with -profile awsbatch
// //nextflow cloud create <your-cloud-name>
// cloud {
//  imageId = 'ami-041b122e4eb7e6585'  //if no longer available, you may have to replace this with your own
//  instanceType = 't2.micro'
//  userName = 'ec2-user'
// }

//GENERATE REPORT https://www.nextflow.io/docs/latest/tracing.html#trace-report
report {
    enabled = true
    file = "${params.infodir}/report.html"
}

//GENERATE TIMELINE https://www.nextflow.io/docs/latest/tracing.html#timeline-report
timeline {
    enabled = true
    timeline.file = "${params.infodir}/timeline.html"
}

//GENERATE PIPELINE TRACE https://www.nextflow.io/docs/latest/tracing.html#trace-report
trace {
    enabled = true
    raw	= true //date and time are reported as milliseconds and memory as number of bytes
    file = "${params.infodir}/trace.tsv"
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}

//GENERATE GRAPH REPRESENTATION OF THE PIPELINE FLOW
dag {
    enabled = true
    file = "${params.infodir}/dag.png"
}

// //SEND NOTIFICATION EMAIL IF ADDRESS(ES) PROVIDED -- use -N email@.... instead
// notification {
//   enabled = params.email.isEmpty() ? false : true
//   to = params.email
// }


workflow.onComplete = {
    println "Pipeline complete"

    def runmeta = [:]
    runmeta['Nextflow']  = [:]
    runmeta['Nextflow']['Version'] = workflow.nextflow.version
    runmeta['Nextflow']['Build'] = workflow.nextflow.build
    runmeta['Nextflow']['Compile timestamp'] = workflow.nextflow.timestamp
    runmeta['Manifest'] = workflow.manifest
    runmeta['Run name'] = workflow.runName
    runmeta['Success'] = workflow.success
    runmeta['Date started'] = workflow.start
    runmeta['Date completed'] = workflow.complete
    runmeta['Duration'] = workflow.duration
    runmeta['Exit status'] = workflow.exitStatus
    runmeta['Execution profile'] = workflow.profile
    runmeta['Execution session UUID'] = workflow.sessionId

    // runmeta['Error message'] = (workflow.errorMessage ?: 'None')
    // runmeta['Error report'] = (workflow.errorReport ?: 'None')
    // runmeta['projectDir'] = workflow.projectDir.toString() //EXCLUDE?
    // // runmeta['Pipeline script path'] = workflow.scriptFile shopuld not be uploaded to pub?
    runmeta['Pipeline script hash ID'] = workflow.scriptId
    if(workflow.repository)
      runmeta['Pipeline repository Git URL'] = workflow.repository
    if(workflow.commitId)
      runmeta['Pipeline repository Git commit'] = workflow.commitId
    if(workflow.revision)
      runmeta['Pipeline Git revision'] = workflow.revision
    if(workflow.container)
      runmeta['Container image'] = workflow.container
    if(workflow.containerEngine)
      runmeta['Container engine'] = workflow.containerEngine
    runmeta['Params'] = params //WARNING - these can be overwritten in main.nf
    runmeta['Command line'] = workflow.commandLine

    //runmeta['Trace'] =
    // println workflow.trace

    // println(runmeta)
    // println(groovy.json.JsonOutput.prettyPrint(groovy.json.JsonOutput.toJson(runmeta)))
    def runmetaJSON = new File("${params.infodir}/runmeta.json")
    runmetaJSON.text = groovy.json.JsonOutput.prettyPrint(groovy.json.JsonOutput.toJson(runmeta))

    //  def command = "tree -h -D all"
    // def proc = command.execute()
    // proc.waitFor()

    // println "Process exit code: ${proc.exitValue()}"
    // println "Std Err: ${proc.err.text}"
    // println "Std Out: ${proc.in.text}"


    // evaluate(new File("$baseDir/conf/ApiCalls.groovy"))
    // apiCalls = new ApiCalls()

  // IF --release requested by the user and execution from GH repo
  if(params.release && workflow.repository && workflow.commitId && workflow.revision) {
    if(workflow.revision ==~ /^v?([0-9]+)\.([0-9]+)\.?([0-9]+)?$/ ) {
      GroovyShell shell = new GroovyShell()
      def apiCalls = shell.parse(new File("$baseDir/groovy/ApiCalls.groovy"))

      releaseArgs = [
        REPO : workflow.repository.replaceFirst("^(http[s]?://github\\.com/|git@github\\.com:)","").replaceFirst("\\.git\$",""),
        COMMIT : workflow.commitId,
        LOCAL_FILES : [
          "${params.infodir}/runmeta.json",
          "${params.infodir}/trace.tsv"
        ],
        RELEASE_TAG: "${workflow.revision}_${workflow.runName}",
        RELEASE_NAME: "Results and metadata for run '${workflow.runName}', revision ${workflow.revision}",
        RELEASE_BODY: "Release created and artefacts uploaded via GH API for run ${workflow.sessionId}, commit ${workflow.commitId}, see attached files for more details."
      ]
      apiCalls.gitHubRelease(releaseArgs)
    } else {
      println "Automated GH release generation only aimed at semantically tagged revisions (e.g. v1.5.4), current revision: ${workflow.revision}"
    }
  }
}



//FROM: https://github.com/SciLifeLab/NGI-smRNAseq/blob/29c41afd45011874ed9920c5065ddff93791e3cf/nextflow.config
// Function to ensure that resource requirements don't go beyond a maximum limit
def check_max(obj, type) {
  if(type == 'memory'){
    if(obj.compareTo(params.max_memory) == 1)
      return params.max_memory
    else
      return obj
  } else if(type == 'time'){
    if(obj.compareTo(params.max_time) == 1)
      return params.max_time
    else
      return obj
  } else if(type == 'cpus'){
    return Math.min( obj, params.max_cpus )
  }
}


