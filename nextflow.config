manifest {
  name = 'REPSET'
  version = 'not set'
  author = 'Rad Suchecki et al.'
  homePage = 'https://github.com/csiro-crop-informatics/repset'
  description = 'Reproducible aligner evaluation workflow'
  defaultBranch = 'master'
  nextflowVersion = '!>=19.10.0' //due to https://github.com/nextflow-io/nextflow/issues/1015 fix which prevents failure when (internal) list of either RNA or DNA aligners empty
}



// // // Global default params
params {
  release = false //if true and GH_TOKEN set in environment and executed from repo then (1) create release, (2) upload results and meta artefacts (3) finalise release to trigger zenodo build
  outdir = "./results"
  infodir = "./flowinfo"
  singularitydir = "./singularity-images"
  publishmode = "copy"
  debug = false  //set true for quicker runs on subset data
  // reads = 'realRNA|simulatedRNA|realDNA|simulatedDNA'

  justvalidate = false // if true exit immediatelly after validation
  mappers = 'all'
  mapmode = 'rna2rna|rna2dna|dna2dna'
  one_thread_idx = 'bwa|dart|gsnap|kallisto|subread' //these mappers do not mutli-thread the indexing process, so lets not over-allocate resources

  //Evlauation params
  allowedDelta = 5 //e.g. use --allowed-delta 100 to treat a read as correctly mapped if a coordinate falls up to 100 bp outside its expected position  
}

//Default inputs for Simulations
includeConfig 'conf/simulations.config'

//Default inputs for BEERS Simulated RNA
includeConfig 'conf/BEERS.config'

//Default params for aligners
includeConfig 'conf/mappers.config'
includeConfig 'conf/mapping_params.config'

//Output files conf
includeConfig 'conf/publish.config'

//Compute requirements, CPUs, RAM, time
includeConfig 'conf/requirements.config'

//Global and process specific container conf
includeConfig 'conf/containers.config'

process {
  cache = 'lenient'
  errorStrategy = { params.debug ? 'finish' : (task.attempt < process.maxRetries ? 'retry' : 'ignore') }
  maxRetries = 2
}

profiles {
  docker {
      docker {
        enabled = true
        fixOwnership = true
    }
  }
  awsbatch {
    aws.region = 'ap-southeast-2'
    process {
      executor = 'awsbatch'
      queue = 'flowq'
    }
    executor {
      awscli = '/home/ec2-user/miniconda/bin/aws'
    }
  }
  slurm {
    process {
      executor = 'slurm'
      scratch = false //why explicit?
      withLabel: 'rscript|rrender' {
        executor =  'local'
      }
    }
  }
  singularity {
    singularity {
      enabled = true
      autoMounts = true
      cacheDir = "${params.singularitydir}"  //when distibuting the pipeline probably should point under $workDir
    }
  }
  singularitymodule {  // Should not be needed if compute nodes inherit env from head node
    process.module = 'singularity/3.2.1' //Cluster specific - update if and as required
  }
}

//GENERATE REPORT https://www.nextflow.io/docs/latest/tracing.html#trace-report
report {
    enabled = true
    file = "${params.infodir}/report.html"
}

//GENERATE TIMELINE https://www.nextflow.io/docs/latest/tracing.html#timeline-report
timeline {
    enabled = true
    timeline.file = "${params.infodir}/timeline.html"
}

//GENERATE PIPELINE TRACE https://www.nextflow.io/docs/latest/tracing.html#trace-report
trace {
    enabled = true
    raw	= true //date and time are reported as milliseconds and memory as number of bytes
    file = "${params.infodir}/trace.tsv"
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}

//GENERATE GRAPH REPRESENTATION OF THE PIPELINE FLOW
dag {
    enabled = true
    file = "${params.infodir}/dag.png"
}


workflow.onComplete = {
    println "Pipeline complete"

    def runmeta = [:]
    runmeta['os']  = [:]
    runmeta['os']['Architecture']  = System.getProperty("os.arch")
    runmeta['os']['Name']  = System.getProperty("os.name")
    runmeta['os']['Version']  = System.getProperty("os.version")
    runmeta['java']  = [:]
    runmeta['java']['VM name']  = System.getProperty("java.vm.name")
    runmeta['java']['VM version']  = System.getProperty("java.vm.version")
    runmeta['java']['Vendor name']  = System.getProperty("java.vendor")
    runmeta['java']['Runtime Environment Version']  = System.getProperty("java.runtime.version")
    runmeta['java']['Version']  = System.getProperty("java.version")

    runmeta['workflow'] = workflow.getProperties()
    // runmeta['params'] = params //WARNING - these can be overwritten in main.nf
    // runmeta['params']['note'] = "params can be overwritten in ${workflow.scriptName}"

    println  !(runmeta['workflow']['nextflow'] instanceof java.util.LinkedHashMap)

    //Preventing stack overflow on Path objects and other  when map -> JSON
    jsonGeneratorOptions = new groovy.json.JsonGenerator.Options()
                .addConverter(java.nio.file.Path) { java.nio.file.Path p, String key -> p.toUriString() }
                .addConverter(Duration) { Duration d, String key -> d.durationInMillis }
                .addConverter(java.time.OffsetDateTime) { java.time.OffsetDateTime dt, String key -> dt.toString() }                
                .addConverter(nextflow.NextflowMeta) { nextflow.NextflowMeta m, String key -> m.toJsonMap() }  //incompatible with Nextflow <= 19.04.0 
                .excludeFieldsByType(java.lang.Class) // .excludeFieldsByName('class')
                // .excludeNulls()
    
    ////Conditional for compatibility with Nextflow <= 19.04.0 
    // if(!(runmeta['workflow']['nextflow'] instanceof java.util.LinkedHashMap))
    // try { 
    //   jsonGeneratorOptions.addConverter(nextflow.NextflowMeta) { nextflow.NextflowMeta m, String key -> m.toJsonMap() } 
    // } catch() {

    // }
                
    jsonGenerator = jsonGeneratorOptions.build()

    def runmetaJSON = new File("${params.infodir}/runmeta.json")
    runmetaJSON.text = groovy.json.JsonOutput.prettyPrint(jsonGenerator.toJson(runmeta))

    //  def command = "tree -h -D all"
    // def proc = command.execute()
    // proc.waitFor()

    // println "Process exit code: ${proc.exitValue()}"
    // println "Std Err: ${proc.err.text}"
    // println "Std Out: ${proc.in.text}"
    // evaluate(new File("$baseDir/conf/ApiCalls.groovy"))
    // apiCalls = new ApiCalls()

  // IF --release requested by the user and execution from GH repo
  if(params.release && workflow.repository && workflow.commitId && workflow.revision && workflow.scriptName == 'main.nf') {
    if(workflow.revision ==~ /^v?([0-9]+)\.([0-9]+)\.?([0-9]+)?$/ ) {
      GroovyShell shell = new GroovyShell()
      def apiCalls = shell.parse(new File("$baseDir/groovy/ApiCalls.groovy"))

      releaseArgs = [
        REPO : workflow.repository.replaceFirst("^(http[s]?://github\\.com/|git@github\\.com:)","").replaceFirst("\\.git\$",""),
        COMMIT : workflow.commitId,
        LOCAL_FILES : [
          // "${params.outdir}/report.html",
          // "${params.outdir}/repset-manuscript.pdf",
          "${params.outdir}/allstats.json",
          "${params.infodir}/runmeta.json",
          "${params.infodir}/trace.tsv"
        ],
        RELEASE_TAG: "${workflow.revision}_${workflow.runName}_${workflow.sessionId}",
        RELEASE_NAME: "${workflow.revision} - results and metadata for run '${workflow.runName}'",
        RELEASE_BODY: "Release created and artefacts uploaded for run '${workflow.runName}', session ID ${workflow.sessionId}, commit ${workflow.commitId}, see assets for more details."
      ]
      apiCalls.gitHubRelease(releaseArgs)
    } else {
      println "Automated GH release generation only aimed at semantically tagged revisions (e.g. v1.5.4), current revision: ${workflow.revision}"
      println "Note that this restriction can be lifted without adversely affecting functionality"
    }
  }
}